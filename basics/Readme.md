# Kubectl
Command line tool for K8s cluster.
It is more powerful in compare to UI or API.
## Useful commands
```
kubectl version

kubectl help
kubectl help get 

kubectl get all      # All reources
kubectl get nodes 

kubectl apply -f <file>.yaml   # Apply configuration file

kubectl create -f <file>.yaml  
kubectl create -f <file>.yaml --record   # Going to try and record our rollout history
kubectl get deployments

kubectl get replicasets

kubectl get pods 
kubectl get pods --watch
kubectl get pods -o wide
kubectl describe pod <pod-name>

kubectl describe deployment <deployment-name>
kubectl get deployment/helloworld -o yaml      # Yaml of Live running deployment. refer:  namespace, spec, Status (status help in debuging) 

kubectl get service
kubectl get service/helloworld -o yaml        # refer spec

kubectl describe service/<service-name>     # refer endpoints

kubectl delete -f <file>.yaml  

kubectl top # current CPU and memory usage for a cluster’s pods or nodes, or for a particular pod or node if specified.
```

## Configuration file
It is YAML based 
- Human friendly data serialization, standard for all programming language
- strict indentation
- store file with code or in git.

It has 3 part
- Metadata  (name, labels)
- Spec  (Attribute are specific to the Kind)
- Status (Automatically generated and added by K8s)

Based on the status, If Desired and Actual state won’t match then it will try to self-heal.
Status gets data from etcd (it holds the current status of any k8s component) \
In Deployment spec, you define template
- template has it's own metadata and spec section
- applies to pod
- blueprint for pod

Labels and Selector helps connecting Deployment to pod.
- labels can be any key-value pair for component.
- Pods get labels through the template blueprint.
- The label is match by the selector.

Note: “---” allow to write multiple files in a single file.

# Minikube
Helps to run kubernetes on local machine for testing/learning.
Master and Node processes run on one machine. (1 node k8s cluster)
It runs through hypervisor ex: virualbox. Which create virtual box on laptop. (node run in that virtual box)
Docker preinstalled.

```
# minikube start
# minikube status
# minikube service <service-name>      # It display table entry and  takes to web browser ip and port
# minikube start --vm-driver=hyperkit --v=7 --alsologtostderr  # Running minikube in debug mode
```

# Kubernetes

## Node
Node serve as worker machine.
It can be physical or virtual machine.
Node should have:
- Kubelete running
- Kube-proxy process running
- Container tooling (ex: Docker)
- Supervisord

You can sepecify nodeSelector for pods. if you want to run pod on specific node.
ex: 201-pod-on-specific-node.yaml

## Pod
It is the smallest unit that you can interact with. (Abstraction over container)
It represents one running process on your cluster. 
Usually one application per pod. 
Each pod gets its own IP. (New-IP on re-creation) \
Note: Pods are ephemeral and disposable. It never self heal nor restarted by schedule it-self. Never create pods just by themselves. Always use high-level construct. \ 
Pod State: Pending, Running, Succeeded, Failed, CrashLoopBackOff
```
kubectl create -f 101-pod.yaml 
kubectl get pods | grep demo-pod 
kubectl exec <pod-name> -it -- /bin/bash    
$ curl http://127.0.0.1
kubectl delete -f 101-pod.yaml  
```

## ReplicaSet
Ensures that specified number of replica for a pod running all the time.

## Deployment
It is blueprint for pods. Abstraction of pods. 
Provides declarative updates for Pods and ReplicaSets.
It helps in Pod management and Scaling.
It helps in deploying Stateless app.
Easy to check status, health and issues. \
Note: Deployment manages ReplicaSets and ReplicaSets manages Pods. \
replica -> Run #pods matching the template \
template -> Create pods using pod definition \
Note: The database cannot be replicated via deployment as they have state so their data need to be shared too.

### Nginx
To showcase pod connectivity.
```
kubectl create -f 101-deployment-nginx.yaml 
kubectl describe service <service-name>   # Verify endpoint, it will match with pod-ip.
kubectl get pods | grep demo-nginx 
kubectl exec <pod-name> -it -- /bin/bash  
$ cat /etc/nginx/conf.d/default.conf    
$ curl http://127.0.0.1
$ curl http://<pods-internal-ip>
kubectl delete -f 101-deployment-nginx.yaml  

[commandline] kubectl create deployment <deployment-name> --image=nginx  # cmd-nxinx-depl
[commandline] kubectl edit deployment <deloyment-name> --image=nginx  # autogenerated configuration with with default value.
[commandline] kubectl delete deployment <deployment-name>

Note: If you want to access on browser then need to use service. (Container port should be present) 
kubectl expose deployment <deployment-name> --type=NodePort
```

### Redis
To show case redis data won't be same in each pod.
```
kubectl create -f 101-deployment-redis.yaml 
kubectl get pods | grep demo-redis 
kubectl exec <pod-name> -it -- sh   
$ redis-cli
 > set a 12
 > keys *
$ ls -ltr /usr/local/bin  
$ ls /data
$ redis-cli -h <pods-internal-ip>
kubectl delete -f 101-deployment-redis.yaml  
```
Note: # kubectl expose deployment <deployment> --type=Nodeport   # Create service construct to expose the webpage 

### Probe
initialDelaySeconds -> length of time to wait for a pod to initialize after pod startup, before applying health checking
timeoutSeconds -> Amount of time to wait before timing out
periodSeconds -> Amount of time to wait before timing out
failureThreshold ->  Kubernetes will try failureThreshold times before giving up and restarting the Pod

 kubelet to determine when to restart a container (for livenessProbe) and used by services and deployments to determine if a pod should receive traffic (for readinessProbe).

### Scale 
```
kubectl create -f 101-deployment-nginx-scale.yaml
kubectl get pods
kubectl scale --replicas=3 deploy/<deployment-name>
kubectl get pods
kubectl delete -f 101-deployment-nginx-scale.yaml
```

### Upgrades and Rollback
On change behind the scene deployment creates a second replica set with new desired state.

```
kubectl create -f 101-deployment-nginx-upgrade.yaml --record
kubectl get rs
kubectl get pods | grep demo-nginx 
kubectl exec <pod-name> -it -- nginx -v  
kubectl get deployments
# Edit deployment file to upgrade version
kubectl apply -f 101-deployment-nginx-upgrade.yaml --record
#Or #kubectl set image deployment/<deployment-name> <container-name>=<new-image> --record=true
kubectl get pods | grep demo-nginx 
kubectl exec <pod-name> -it -- nginx -v  
kubectl get rs       # Notice 2 ReplicaSet - New with desired state

kubectl rollout history deployment/<deployment-name>  # To see history

kubectl rollout undo deployment/<deployment-name>    # To revert the changes
kubectl rollout undo deployment/<deployment-name> --to-revision=version  # To revert to sepcific version

kubectl delete -f 101-deployment-nginx-upgrade.yaml  
```

## Service
Allow communication between one set of deployments with others.
It gets a unique permanent IP address that never changes through the lifetime of the service.
Service allows one set of pods to communicate with another set of pods in an easy way.
Lifecycle of pod and service not connected.
Serivce also provide functionality of load balancing.
Type:
- Internal: Ip is only reachable within the cluster.
- External: Endpoint available through nodeIP:nodePort. 
- LoadBalancer:Exposes application to the internet with a cloud load-balancer. 
In configuration file, targetPort should be same to container port.
Note: nodePort value muse be in between 30000-32767
Note: Internal service or ClusterIP is Default.
 
### Nodeport
```
kubectl create -f 101-deployment-nginx.yaml
kubectl create -f 101-service-nodeport.yaml 
kubectl get services 
# curl http://127.0.0.1:30534
kubectl delete -f 101-deployment-nginx.yaml
kubectl delete -f 101-service-nodeport.yaml
```
### Loadbalancer
If supported it automatically create an load-balancer with external IP for the frontend service.
```
kubectl create -f 101-deployment-nginx.yaml
kubectl create -f 101-service-loadbalancer.yaml 
kubectl get services 
# curl http://127.0.0.1:8080
kubectl delete -f 101-deployment-nginx.yaml
kubectl delete -f 101-service-loadbalancer.yaml 
```

## Ingress
Ingress do the forwarding to the service. 
Instead of exposing IP:port by making service external, you can use Ingress (defining host name, etc)
It can help to achieve domain based access and secure connection. 
With Ingress we don't need to expose service externally (so no nodeport or loadbalancer).
Host:
- Valid domain address
- map domain name to Node's IP address, which is the entrypoint. (it can be outsider the cluster)

How to configure Ingress in cluster?
You need an implementation for ingress which is ingress controller.
Ingress controller:
- evaluates and processes ingress rules, 
- manages redirection, 
- entrypoint to cluster.
- many third part implementation
- K8s Nginx Ingress controller

To test locally use /etc/host to resolve IP for domain to redirect to different application.


(One domain multiple service) You can define multiple path for host in Ingress.
(multiple sub-doamains and one path) You can define multiple host with one path. Each host represent subdomain.
You can configure TLS certificate for https. Certificate will be stores in secrets (Specific type tls used).
Configuring TLS Certificate:
- Data key eed to be "tls.crt" and "tls.key"
- Values are file contents NOT file paths/locations
- Secret component must be in the same namespace as the  ingress component.
```
kubectl create -f 101-ingress.yaml
kubectl get ingress 

# ingress controller in minikube
minikube addons enable ingress
```

## Accessing service from other pods
FQDN: $SVC.$NAMESPACE.svc.cluster.local
```
kubectl exec <pod-nam> -it -c shell -- ping <service-name>.<namespace>.svc.cluster.local   # default -> if no namespace defined
kubectl exec <pod-nam> -it -c shell -- curl http://<service-name>/info
kubectl exec <pod-nam> -it -c shell -- curl http://<service-name>.<namespace>/info 


```

## Environment Variable

```
kubectl create -f 101-env.yaml 
kubectl get pods
kubectl exec demo-env-usage -t -- curl -s 127.0.0.1:80 
kubectl exec demo-env-usage -- printenv 
kubectl delete -f 101-env.yaml 
```

## Multi-container in pods
```
kubectl apply -f 101-pod-multi-container.yaml 
kubectl get pods
kubectl logs <pod-name> 
kubectl delete -f 101-pod-multi-container.yaml  
```

## Secrets 
Handling sensitive/secret data for application. ex: Db password, api tokens
Writing these directly in deployment yaml etc make it visible to others.
Secret encoded in base64. (Storing the data in secret component doesn't automatically make it secure. The built-in security mechanism (like encryption) is not enabled by default)
Secret can be use as environment variables or as a properties file. 
You can access secrets via a volume. The secret data on nodes is stored in tmpfs volumes.
Opaque -> default for arbitrary key pair
Note: To generate base64 data use command # echo -n "<data>" | base64
```
[commandliine] #kubectl create secret generic <secret-name> --from-literal=<name>=<value>

kubectl create -f 101-secret.yaml
kubectl get secrets
kubectl get secrets <secret-name> -o yaml

kubectl create -f 101-secret-usage.yaml 
kubectl get pods
kubectl exec <pod-name> -it -- /bin/bash
echo $SECRET_USERNAME
echo $SECRET_Password

kubectl create -f 101-secret-using-volume.yaml
kubectl get pods
kubectl exec <pod-name> -it -- /bin/bash
mount | grep <secret-dir>
ls -ltr /tmp/<secret-dir>/
cat /tmp/<secret-dir>/<key>

kubectl delete -f 101-secret-using-volume.yaml
kubectl delete -f 101-secret-usage.yaml
kubectl delete -f 101-secret.yaml 
```
 

## Configmap
It is external configuration of your application.
You can pass data to application that can be changed at deploy time. ex: Loglevel, endpoint to external system
Instead of hard-coding values, use configmaps to pass values as environment variables to the container.
Don't put credential in the configmap use secret. 
It can be used by more than one component.
```
[commandliine] #kubectl create configmap <configmap-name> --from-literal=<name>=<value>

kubectl create -f 101-configmap.yaml
kubectl get configmap
kubectl get configmap <configmap-name> -o yaml

kubectl create -f 101-configmap-usage.yaml 
kubectl get pods
kubectl exec <pod-name> -it -- /bin/bash
echo $SERVICE_NAME

#kubectl edit configmap <configmap-name> 

kubectl delete -f 101-configmap-usage.yaml
kubectl delete -f 101-configmap.yaml 
```

## Jobs
Runs individual process that run once and complete successfully.
The output of the job is kept around until you decide to remove it.
Mostly, Jobs are run as a cron job to run a specific process at a specific time and repeat it at another time.
restartPolicy: Always|OnFailure|Never
### Jobs
```
kubectl create -f 101-job-simple-job.yaml 
kubectl get jobs 
kubectl get pods | grep demo-cron-job    # Not in READY state but STATUS Completed.
kubectl logs <pod-name>
kubectl desribe jobs/<job-name>
kubectl delete -f 101-job-simple-job.yaml  
``` 

### CronJob
these are like jobs, however, they can run periodically. 
To setup cronjob use cron syntax.
You can use short syntax @yearly, @annually, @monthly, @weekly, @daily, @midnight, @hourly.
```
kubectl create -f 101-job-cron-job.yaml 
kubectl get jobs 
kubectl get cronjob
kubectl get pods | grep demo-cron-job 
kubectl logs <pod-name>
kubectl edit cronjob/<cron-name>
kubectl delete -f 101-job-cron-job.yaml  
```
Run 'kubectl get jobs' and 'kubectl get cronjobs' periodically in short interval to see changes. 
Default 3 completed jobs and 1 failed job maintained as history (See #Pods).
To stop the cronjob set 'suspend: true' by editing the crontab. 
By default Suspend set as false. This just means that don't ever suspend it, just always keep running it.
Active highlight if it's active right now.


## Labels
Key/Value pair attached to object (ex: pod, serivce, deployment, etc).
It help to identify attributes of objects. 
Labels used with Label-selectors to give better visibility. 
Type of Label-Selectors:
- Equity based: =  !=
- Seb-based: in  notin  exists

```
kubectl create -f 101-labels.yaml
kubectl get pods --show-labels
kubectl label pods/<pod-name>  sensitive=false --overwrite     # To add label on running pod eg: addnig label sensitive
kubectl get pods <pod-name>  --show-labels 

# Delete labels from pod
kubectl label pods/<pod-name> sensitive-     # To delete label from running pod eg: deleting label sensitive

# Use selector to search with labels. Shortcut is -l.
kubectl get pods --selector env=dev,app_type=web
kubectl get pods --selector env!=dev,app_type=web
kubectl get pods --selector "env in (dev,uat)"
kubectl get pods --selector "env notin (uat,prod)" --show-labels
kubectl get pods --selector "env in (dev,uat),app_type in (web,backend)"
# Deleting resources based on labels
kubectl delete pods -l project=demo-lables

kubectl delete -f 101-labels.yaml
```

## Namespaces
It is a feature of Kubernetes that allows you to have multiple virtual clusters backed by the same physical cluster. 
It is a fundamental concept to add multi-tenancy to your Kubernetes instance.
Helpful for large enterprise.
Names of resources, like deployments and pods, must be unique within the namespace, but not necessarily across separate namespaces. \
By Default following namespaces are present:
- kube-system (System process, Master and Kubectl process, Do not create/modify in kube-system)
- kube-public (Public accessible data, a configmap which contain cluster info. #kubectl cluster-info)
- kube-node-lease (heartbeat of nodes, each node has associated lease object, determines the availability of node)
- default (resources you create are located here )  

Why to use namespace?
- Structure your components: Without custom namespaces everything will be in default which will not give better insight. 
  Better to group resources. ex: Databases, Monitoring, Ingress, Logging stack
- Avoid conflicts between teams: Conflicts can happen (many team, same application -> override)
  Better to group resources based on Project.
- Share services between different environment: With namespace you can re-use components (resource sharing) for different environments. 
  Ex: Dev and UAT can share common resources Ingress, Logging stack
  Also, resource sharing possible for blue/green deployments. 
- Access and Resource limit: With namespace, you can add access and resource limit (CPU, RAM, Storage) on namespace. 
  So each team will have isolated environment. 
Characteristics:
- You cannot access most resources from another Namespace. ex: Configmap, Secrets
- You can access services from other namespace. You need to specify the namespace-name with service.
- Few components live globally, which cann't be created within namespace. ex: vol, node
```
kubectl get namespaces
kubectl describe ns default
kubectl create -f 101-namespace.yaml
kubectl get namespaces
kubectl get pods --namespace=<namespace-name>
kubectl delete -f 101-namespace.yaml

[command-line] kubectl create namespace <namespace-name>
[command-line] kubectl delete namespace <namespace-name>

kubectl api-resources --namespaced=false   # To see component that cannot be created in namespace
```
Note: Use -n <namespace-name> while creating resources (eg: deployment) in specific namespace.

To avoid using namespace flag in each command, change active namespace.
K8s don;t provide out of the box solution.
Change the active namespace with kubens
```
brew install kubectx
kubens
kubens <namespace-name>
```


## Volumes
A directory accessible to all containers running in a pod.
In contrast to the container-local filesystem, the data in volumes is preserved across container restarts.
The medium backing a volume and its contents are determined by the volume type:
- node-local types such as emptyDir or hostPath
- file-sharing types such as nfs
- cloud provider-specific types like awsElasticBlockStore, azureDisk, or gcePersistentDisk
- distributed file system types, for example glusterfs or cephfs
- special-purpose types like secret, gitRepo
Note: K8s doesn't manage data persistance. 
```
kubectl create -f 101-volume-shared.yaml
kubectl get pod
kubectl describe pod <pod-name>  # refer Volumes
kubectl exec <pod-id> -it -c <container-name> -- /bin/bash  # container 1
mount | grep <shared>
echo 'my data' > /tmp/shared-dir/my_data
kubectl exec <pod-id> -it -c <container-name> -- /bin/bash  # container 2 
mount | grep <shared>
cat /tmp/shared-data/my_data

```
In each container you need to decide where to mount the volume.
For emptyDir you currently can not specify resource consumption limits.

## Persists data
No data persistence out of the box. (On restart data loss)
Requirement:
- Storage that doesn't depend on the pod lifecycle.
- Storage must be available on all nodes.
- Storage needs to survive even if cluster crashes.
Usecases:
- DB data
- App read/write file from directory 

Note: Config map and secret volumes managed by Kubernetes. It is not created by PV and PVC.

### Persistent Volumes
a cluster-wide resource that you can use to store data in a way that it persists beyond the lifetime of a pod. 
The PV is not backed by locally-attached storage on a worker node but by networked storage system such as EBS or NFS or a distributed filesystem like Ceph.
https://kubernetesbyexample.com/pv/
PV is abstract storage so it need actual physical storage like (local-disk, nfs server, cloud-storage)
Consider storage as external plugin to your cluster. You need to create and manage them by yourself.
Application can use any of storage or multiple.

spec section covers physical storage.  
spec attribute differ based on the storage type.
PV volumes are not namespaced. It is available to whole cluster.
Each volume type has it's own use case. 
Local volume type violates requirement not tied to specific node, surviving cluster crash.
For DB persistence use remote storage.

Storage resources is provisioned by admin and used by developer using claim. (Developer need to  inform how much storage needed.)

### Persistent Volume Claim
Pod request the volume through PV claim. 
Claim tries to find a volume cluster.
Claim must be in the same namespace.

### Storage Class
If you have many applications and need persistent storage often. 
Asking admin to provide storage is a tedious process.
Storage class make process more efficient.
SC provisions persistent volumes dynamically when PVC claims it.

StorageBackend is defined in the SC component via provisioner attribute.
Each storage backend has own provisionser.
Internal provisioner - "kubernetes.io"
external provisiones
configure parameters for storage we want to request for PV

Requested by PVC.
- Pod claims storage via PVC
- PVC requests storage from SC
- SC creates PV that meets the needs of the claim


## Troubleshooting
Observer deployment, pods 'Not Ready' but pod is running. Verify more information about pod.
```
kubectl create -f 101-troubleshoot-bad-readiness.yaml   
kubectl get pods
kubectl describe pod <pod-name>   # Check for events
kubectl delete -f 101-troubleshoot-bad-readiness.yaml 
```

Observer deployment, pods 'Ready' initially. but pod is restarting 1 then 2 then 3. 
Then pod will move to “CrashLoopBackoff” status and not Not Ready but pod is running so get more info about pod. 
Also, deployment status will change to not ready.
```
kubectl create -f 101-troubleshoot-bad-liveness.yaml   
kubectl get pods
kubectl describe pod <pod-name>   # Check for events
kubectl delete -f 101-troubleshoot-bad-liveness.yaml 
```
Observer deployment (0 Available), pods (0 on ready status) with status ErrImagePull
```
kubectl create -f 101-troubleshoot-bad-pod.yaml  
kubectl describe deployment <deployment-name> # Check for events - Ok
kubectl get pods
kubectl describe pod <pod-name>   # Check for events
kubectl delete -f 101-troubleshoot-bad-pod.yaml
```
```
# More options
kubectl logs <pod-id>
kubectl exec <pod-id> -it -- /bin/bash   # You can see process, environment variable, et
kubectl exec <pod-id> -it -c <container-name> -- /bin/bash  # If using more than one container in pod 
```



